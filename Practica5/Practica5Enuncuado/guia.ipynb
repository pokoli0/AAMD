{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "060c0eca",
   "metadata": {},
   "source": [
    "# Guia para hacer un modelo de ia del examen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799919a5",
   "metadata": {},
   "source": [
    "## Ejercicio 1: Limpiar los datos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66b4536",
   "metadata": {},
   "source": [
    "### QUITANDO FILAS Y COLUMNAS Y NORMALIZANDO DATOS\n",
    "\n",
    "Para normalizar los datos hay que tener en cuenta que:\n",
    "    \n",
    "    Discretos/Enumerados/Cualitativos -> OneHotEncoding o Labled Encoder\n",
    "        OneHotEncoding -> por defecto, si dudas usa este\n",
    "        LabledEncoder -> si los valores tienen un orden\n",
    "\n",
    "    Continuos -> StandardScaling, para normalizarlos y llevarlos a una escala comun\n",
    "\n",
    "\n",
    "\n",
    "    -> solo deberia hacer falta copiar este codigo y cambiar las columnas en ATRIBUTOS, en final_data salen todos los datos sin el atributo solucion y en labeled_data el solucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3a95d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import glob as glob\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "\n",
    "\n",
    "# ---------------------------- / LECTURA DE DATOS / ----------------------------------\n",
    "data = pd.read_csv(\"dementia_dataset.csv\")\n",
    "\n",
    "# TRUCO: Antes de rellenar las listas de abajo, haz un print(data.info()) \n",
    "# y un print(data.describe(include='all')) en una celda aparte para ver qué columnas hay.\n",
    "\n",
    "\n",
    "\n",
    "# ---------------- / GESTIÓN DE NULOS / ------------------- \n",
    "# ----- DIAGNOSTICO\n",
    "# print(\"--- INFO ---\")\n",
    "# print(data.info())\n",
    "# print(\"\\n--- NULOS POR COLUMNA ---\")\n",
    "# print(data.isnull().sum()) # ¡ESTO ES CRUCIAL!\n",
    "# Si ves una columna con 50% de nulos -> A la lista 'unnecessary_columns'\n",
    "# Si ves una columna con 5% de nulos -> Imputar (media) o Borrar filas\n",
    "\n",
    "# Cuantos datos perderiamos: \n",
    "    # Si se pierden 10-15 filas OPCION A\n",
    "    # si se pierden 50-100 filas OPCION B\n",
    "\n",
    "# ----- QUITAR / MODIFICAR NULOS\n",
    "# OPCIÓN A : Borrar filas con huecos.\n",
    "# Úsalo si tienes muchos datos (>1000) y pocos huecos.\n",
    "#final_data = data.dropna(how=\"any\")\n",
    "\n",
    "# OPCIÓN B (Alternativa si tienes pocos datos): IMPUTAR\n",
    "# Si ves que al hacer dropna te quedas con muy pocas filas, usa esto antes de escalar:\n",
    "# data['SES'] = data['SES'].fillna(data['SES'].mean()) # Imputa la media en los huecos \n",
    "# data['...'] = data['...'].fillna(data['...'].median()) # Imputa la mediana en los huecos \n",
    "# Verificamos que ya no quedan nulos\n",
    "print(\"Nulos restantes:\", data.isnull().sum().sum())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------- / ATRIBUTOS / ---------------------\n",
    "unnecessary_columns = [\"Subject ID\", \"MRI ID\", \"Hand\"]\n",
    "oneHot_columns = [\"M/F\"]\n",
    "standardScaling_columns = [\"Age\", \"EDUC\", \"SES\", \"MMSE\", \"CDR\", \"eTIV\", \"nWBV\", \"ASF\", \"Visit\", \"MR Delay\"]\n",
    "labeled_columns = [\"Group\"] # SOLUCION\n",
    "\n",
    "\n",
    "\n",
    "# ---------------- / DROP /  -------------------\n",
    "# Borramos lo que no sirve para limpiar el ruido del dataset.\n",
    "final_data = data.drop(columns=unnecessary_columns) \n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------- / OHE /  -------------------\n",
    "encoder = OneHotEncoder(sparse_output=False) \n",
    "encoder_final = encoder.fit_transform(data[oneHot_columns])  \n",
    "# Creamos un DF temporal con nombres bonitos (ej: \"M/F_M\", \"M/F_F\")\n",
    "oneHot_df = pd.DataFrame(encoder_final, columns=encoder.get_feature_names_out(oneHot_columns))\n",
    "\n",
    "\n",
    "# ---------------- / SCALER /  -------------------\n",
    "# Transforma los datos para que tengan media 0 y desviación típica 1 (Curva de Gauss).\n",
    "scaler = StandardScaler()\n",
    "scaler_final = scaler.fit_transform(data[standardScaling_columns])\n",
    "df_sc = pd.DataFrame(scaler_final, columns=standardScaling_columns, index=data.index)\n",
    "\n",
    "\n",
    "\n",
    "# ----------------- / LABELED ENCODER / -------------------------   \n",
    "# Convierte \"Demented\" -> 0, \"Nondemented\" -> 1, etc.\n",
    "labler = LabelEncoder()\n",
    "labeled_final = labler.fit_transform(data[labeled_columns].values.ravel()) # o labeled_final = labler.fit_transform(data[labeled_columns])\n",
    "df_lbl = pd.DataFrame(labeled_final, columns=labeled_columns, index=data.index)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# ---------------- / CONCATENAR (SOLUCION AL FINAL) / -------------------\n",
    "# ---- 1. Juntamos las 3 partes: Numéricos Escalados + Categóricos OHE + Solución Codificada\n",
    "final_data = pd.concat([df_sc, oneHot_df, df_lbl], axis=1) #axis=1 significa \"pegar columnas a la derecha\"\n",
    "\n",
    "\n",
    "final_data.head()\n",
    "\n",
    "# ---------------- / OPCIONAL / -------------------\n",
    "# # Opcional: prints para verificar\n",
    "# print(\"Tamaño final del dataset:\", final_data.shape)\n",
    "# # para guardarlas en un archivo (opcional)\n",
    "# final_data.to_csv(\"./examen_limpio.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5dcc29",
   "metadata": {},
   "source": [
    "### Slices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4840621b",
   "metadata": {},
   "source": [
    "La sintaxis de slicing ([:]) es la herramienta número 1 para trocear tus datos. Casi siempre tendrás un CSV donde la última columna es la solución (ej: HeartDisease o Group). Usando pandas: \n",
    "* dataset.iloc[:, :-1] $\\to$ Coge TODAS las filas (:) y TODAS las columnas MENOS la última (:-1). $\\to$ Esto es tu X.dataset.\n",
    "* iloc[:, -1] $\\to$ Coge TODAS las filas (:) y SOLO la última columna (-1). $\\to$ Esto es tu y.\n",
    "\n",
    "#### Ejemplo práctico de examen\n",
    "1. Separar features y target\n",
    "* X = final_data.iloc[:, :-1]  # Todo menos la solución\n",
    "* y = final_data.iloc[:, -1]   # Solo la solución\n",
    "\n",
    "2. Dividir datos manualmente (Train / Test)\n",
    "Si no te dejan usar train_test_split de sklearn (raro, pero posible) o si son series temporales (donde no puedes mezclar aleatoriamente):\n",
    "* datos[:800] $\\to$ Coge desde el principio hasta la fila 800 (Entrenamiento).\n",
    "* datos[800:] $\\to$ Coge desde la fila 800 hasta el final (Test).\n",
    "\n",
    "3. Imágenes (CNNs)\n",
    "Si te cae algo de imágenes (matrices 3D: Alto, Ancho, Canales):\n",
    "* imagen[:, ::-1] $\\to$ Invierte la imagen horizontalmente (Efecto espejo para Data Augmentation).\n",
    "* imagen[10:100, 10:100] $\\to$ Recorta la imagen (Crop) para quitar bordes inútiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33d9b079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 6, 5, 4, 3, 2, 1, 0]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "start = 0\n",
    "stop = 2\n",
    "step = 1\n",
    "\n",
    "a[start:stop]  # items start through stop-1\n",
    "a[start:]      # items start through the rest of the array\n",
    "a[:stop]       # items from the beginning through stop-1\n",
    "a[:]           # a copy of the whole array\n",
    "\n",
    "# ---------------------------------------------\n",
    "\n",
    "a[start:stop:step] # start through not past stop, by step\n",
    "\n",
    "# ---------------------------------------------\n",
    "\n",
    "a[-1]    # last item in the array\n",
    "a[-2:]   # last two items in the array\n",
    "a[:-2]   # everything except the last two items\n",
    "\n",
    "# ---------------------------------------------\n",
    "\n",
    "a[::-1]    # all items in the array, reversed\n",
    "a[1::-1]   # the first two items, reversed\n",
    "a[:-3:-1]  # the last two items, reversed\n",
    "a[-3::-1]  # everything except the last two items, reversed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35a2ba7",
   "metadata": {},
   "source": [
    "## Ejercicio 2: Representar los datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e059d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# -------------------- / PREPARACIÓN / -------------------\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m x \u001b[38;5;241m=\u001b[39m final_data                          \u001b[38;5;66;03m# x: Son tus datos limpios (sin la solución). Ya lo tienes del Ejercicio 1.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m y \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m=\u001b[39m labeled_data\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mravel()     \u001b[38;5;66;03m# y: Es la columna de solución. Úsala directa de 'labeled_data' (Ejercicio 1).\u001b[39;00m\n\u001b[0;32m      9\u001b[0m                                         \u001b[38;5;66;03m# .values.ravel() asegura que sea un array plano y no de problemas.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# clase: TIENE QUE SER UN STRING, NO UNA LISTA\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'final_data' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# -------------------- / PREPARACIÓN / -------------------\n",
    "x = final_data                          # x: Son tus datos limpios (sin la solución). Ya lo tienes del Ejercicio 1.\n",
    "y = df_lbl.values.ravel()     # y: Es la columna de solución. Úsala directa de 'labeled_data' (Ejercicio 1).\n",
    "                                        # .values.ravel() asegura que sea un array plano y no de problemas.\n",
    "\n",
    "# clase: TIENE QUE SER UN STRING, NO UNA LISTA\n",
    "clase = \"Group\"        # variable solucion\n",
    "\n",
    "color = y  #codificando cada action como un color\n",
    "\n",
    "\n",
    "\n",
    "# ---------------- / DISTRIBUCIÓN DE CLASES / -------------------\n",
    "# Úsalo si piden \"Ver distribución de clases\" o \"Balanceo del dataset\"\n",
    "print(f\"--- Distribución de {clase} ---\")\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# TRUCO PRO: Usamos 'data' (el original) en vez de 'labeled_data' para que\n",
    "# en el gráfico salgan los nombres (\"Demented\") en vez de números (0, 1).\n",
    "# Si usas 'labeled_data' saldrán 0, 1, 2.\n",
    "                                      \n",
    "data[clase].value_counts().plot(kind='bar', color=['skyblue', 'orange']) # pon mas colores si hace falta\n",
    "plt.title(f\"Distribución de la variable objetivo: {clase}\")\n",
    "plt.xlabel(\"Clases\")\n",
    "plt.ylabel(\"Cantidad de pacientes\")\n",
    "plt.xticks(rotation=0) # Para que las letras se lean rectas\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ------------------- / PCA (Reducción) / ------------------------ \n",
    "pca_comp = PCA(n_components=2)\n",
    "x_pca = pca_comp.fit_transform(x)\n",
    "\n",
    "# Cuanta info mantenemos:\n",
    "print(f\"Varianza explicada: {pca_comp.explained_variance_ratio_.sum():.2f}\") \n",
    "\n",
    "df_pca = pd.DataFrame({\n",
    "    \"c1\": x_pca[:, 0],    # Componente Principal 1\n",
    "    \"c2\": x_pca[:, 1],    # Componente Principal 2\n",
    "    clase: y                # el color (0,1,2...)\n",
    "})\n",
    "\n",
    "grupos = sorted(df_pca[clase].unique()) # [0, 1, 2]\n",
    "    \n",
    "    \n",
    "\n",
    "# ---------------------------- / PINTADO / ------------------------ \n",
    "# Colores: Usamos un mapa de colores automático\n",
    "colormap = plt.get_cmap(\"viridis\")\n",
    "colors = colormap(np.linspace(0, 1, len(grupos)))\n",
    "\n",
    "for i, g in enumerate(grupos):\n",
    "    # Filtramos los puntos de ese grupo\n",
    "    subset = df_pca[df_pca[clase] == g]\n",
    "    \n",
    "    # --- ETIQUETAS REALES (Demented vs 0) ---\n",
    "    # Intentamos recuperar el nombre real usando el 'labler' del Ejercicio 1.\n",
    "    try:\n",
    "        nombre_real = labler.inverse_transform([g])[0]\n",
    "    except:\n",
    "        nombre_real = f\"Clase {g}\" # Si falla, ponemos Clase 0, Clase 1...\n",
    "    \n",
    "    plt.scatter(\n",
    "        subset[\"c1\"], \n",
    "        subset[\"c2\"],\n",
    "        color=colors[i],\n",
    "        alpha=0.7,\n",
    "        s=80,             # Tamaño del punto\n",
    "        label=nombre_real # ¡Importante para la leyenda!\n",
    "    )\n",
    "\n",
    "plt.legend(title=\"Estado del Paciente\")\n",
    "plt.title(\"PCA: Visualización de datos tras limpieza\")\n",
    "plt.xlabel(f\"Componente 1 ({pca_comp.explained_variance_ratio_[0]*100:.1f}% var)\")\n",
    "plt.ylabel(f\"Componente 2 ({pca_comp.explained_variance_ratio_[1]*100:.1f}% var)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79fde29",
   "metadata": {},
   "source": [
    "## Ejercicio 3: MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27c595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EJERCICIO 3: ENTRENAMIENTO Y EVALUACIÓN DEL MLP\n",
    "# ==============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. COPIA AQUÍ TU CLASE 'MultilayerPerceptron' ENTERA\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "class MultilayerPerceptron:\n",
    "    def __init__(self, layers, learning_rate=0.1, epochs=5000):\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.loss_history = []\n",
    "        \n",
    "        np.random.seed(42) \n",
    "        for i in range(len(layers) - 1):\n",
    "            limit = np.sqrt(6 / (layers[i] + layers[i+1]))\n",
    "            w = np.random.uniform(-limit, limit, (layers[i], layers[i+1]))\n",
    "            b = np.zeros((1, layers[i+1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoid_derivative(self, a):\n",
    "        return a * (1 - a)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True)) \n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        activations = [X]\n",
    "        input_data = X\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(input_data, self.weights[i]) + self.biases[i]\n",
    "            a = self.sigmoid(z) \n",
    "            activations.append(a)\n",
    "            input_data = a\n",
    "        z_last = np.dot(input_data, self.weights[-1]) + self.biases[-1]\n",
    "        activations.append(self.softmax(z_last))\n",
    "        return activations\n",
    "\n",
    "    def backpropagation(self, X, y_onehot, activations):\n",
    "        m = X.shape[0]\n",
    "        deltas = [activations[-1] - y_onehot]\n",
    "        for i in range(len(self.weights) - 2, -1, -1):\n",
    "            delta_prev = np.dot(deltas[-1], self.weights[i+1].T) * self.sigmoid_derivative(activations[i+1])\n",
    "            deltas.append(delta_prev)\n",
    "        deltas.reverse()\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * (np.dot(activations[i].T, deltas[i]) / m)\n",
    "            self.biases[i] -= self.learning_rate * (np.sum(deltas[i], axis=0, keepdims=True) / m)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y = y.astype(int) \n",
    "        y_onehot = np.eye(len(np.unique(y)))[y]\n",
    "        print(f\"Entrenando MLP Propio: {self.layers}\")\n",
    "        for epoch in range(self.epochs):\n",
    "            activations = self.forward(X)\n",
    "            self.backpropagation(X, y_onehot, activations)\n",
    "            if epoch % (self.epochs // 10) == 0:\n",
    "                loss = -np.mean(np.sum(y_onehot * np.log(activations[-1] + 1e-8), axis=1))\n",
    "                self.loss_history.append(loss)\n",
    "                print(f\"   Epoch {epoch}: Loss {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.forward(X)[-1], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0397fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 2. PREPARACIÓN DE DATOS (CRUCIAL: PANDAS -> NUMPY)\n",
    "# ---------------------------------------------------------\n",
    "# Tu clase no entiende DataFrames, solo Arrays de Numpy.\n",
    "# 'final_data' y 'df_lbl' vienen de tu limpieza previa.\n",
    "\n",
    "# Convierte DataFrame a Matriz Numpy\n",
    "X_numpy = final_data.values     \n",
    "# Convierte Series a Array Numpy ---\n",
    "y_numpy = df_lbl.values.ravel()# Usamos .ravel() para asegurar que sea [0, 1, 2...] y no [[0], [1], [2]...]\n",
    "\n",
    "\n",
    "print(f\"Shape de X: {X_numpy.shape}\")\n",
    "print(f\"Shape de y: {y_numpy.shape}\") # Debe poner (N,) y NO (N, 1)\n",
    "\n",
    "# División Train / Test (80% - 20%)\n",
    "# stratify=y_numpy es OBLIGATORIO en exámenes para que las clases estén balanceadas en el test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_numpy, y_numpy, test_size=0.2, random_state=42, stratify=y_numpy\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. CONFIGURACIÓN DE LA RED\n",
    "# ---------------------------------------------------------\n",
    "input_dim = X_train.shape[1]         # Número de neuronas de entrada (automático)\n",
    "output_dim = len(np.unique(y_numpy)) # Número de clases/salidas (automático)\n",
    "\n",
    "# EL ENUNCIADO PIDE: \"Prueba con más de una capa oculta\".\n",
    "# Estructura: [Entrada, Oculta1, Oculta2, ..., Salida]\n",
    "# Recomendación examen: Empieza con algo potente como [input, 64, 32, output]\n",
    "layer_structure = [input_dim, 64, 32, output_dim] \n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. INSTANCIAR Y ENTRENAR\n",
    "# ---------------------------------------------------------\n",
    "print(f\"--- Entrenando Modelo con estructura: {layer_structure} ---\")\n",
    "\n",
    "# CONSEJOS HIPERPARÁMETROS SI EL ACCURACY ES BAJO:\n",
    "# 1. learning_rate: Prueba 0.1, 0.01, 0.001. Si es muy alto oscila, si es muy bajo no aprende.\n",
    "# 2. epochs: Sube a 10000 o 20000 si la curva de loss sigue bajando.\n",
    "mi_mlp = MultilayerPerceptron(layers=layer_structure, learning_rate=0.1, epochs=10000)\n",
    "\n",
    "mi_mlp.fit(X_train, y_train)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. EVALUACIÓN Y RESULTADOS\n",
    "# ---------------------------------------------------------\n",
    "# Predicción\n",
    "y_pred = mi_mlp.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\n>>> ACCURACY CONSEGUIDO: {acc*100:.2f}%\")\n",
    "\n",
    "if acc < 0.65:\n",
    "    print(\"⚠️ ALERTA: No llegas al 65%. Sube epochs, baja learning_rate o añade neuronas.\")\n",
    "else:\n",
    "    print(\"✅ OBJETIVO CUMPLIDO (>65%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d9a247",
   "metadata": {},
   "source": [
    "#### Modelo definitivo:\n",
    "El modelo seleccionado es una red con arquitectura [X, 64, 32, Y] con learning_rate=0.1 y 10000 epochs porque ha conseguido el mejor equilibrio entre convergencia y precisión en test (superando el 65%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1abdfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 6. GRÁFICAS (CURVA + MATRIZ DE CONFUSIÓN)\n",
    "# ---------------------------------------------------------\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Gráfica 1: Curva de Pérdida (Loss)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(mi_mlp.loss_history)\n",
    "plt.title(\"Curva de Aprendizaje (Loss)\")\n",
    "plt.xlabel(\"Iteraciones\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Gráfica 2: Matriz de Confusión (Bonita)\n",
    "plt.subplot(1, 2, 2)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title(\"Matriz de Confusión\")\n",
    "plt.xlabel(\"Predicción del Modelo\")\n",
    "plt.ylabel(\"Valor Real\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# (Opcional) Informe detallado por clase\n",
    "print(\"\\n--- INFORME DETALLADO ---\")\n",
    "# Si tienes el 'labler' del ejercicio 1, usa target_names para que salgan los nombres reales\n",
    "try:\n",
    "    nombres_clases = labler.classes_.astype(str)\n",
    "    print(classification_report(y_test, y_pred, target_names=nombres_clases))\n",
    "except:\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec92073f",
   "metadata": {},
   "source": [
    "## Ejercicio 4: Otros modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac748f5",
   "metadata": {},
   "source": [
    "### MLP SKlearn\n",
    "\n",
    "   | pros | cons | \n",
    "   |---|---|\n",
    "   | tolerancia a ruido | caja negra |\n",
    "   | lineal y no lineal | costoso |\n",
    "   | escalable | --- |\n",
    "\n",
    "Úsala si piden replicar una red clásica o si te piden parámetros específicos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386d3231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ines\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.32073173\n",
      "Iteration 2, loss = 3.96235383\n",
      "Iteration 3, loss = 1.06117503\n",
      "Iteration 4, loss = 0.95524426\n",
      "Iteration 5, loss = 0.92094986\n",
      "Iteration 6, loss = 0.91462271\n",
      "Iteration 7, loss = 0.91362414\n",
      "Iteration 8, loss = 0.90979727\n",
      "Iteration 9, loss = 0.89987311\n",
      "Iteration 10, loss = 0.88481297\n",
      "Iteration 11, loss = 0.86890031\n",
      "Iteration 12, loss = 0.85230531\n",
      "Iteration 13, loss = 0.82924883\n",
      "Iteration 14, loss = 0.79212821\n",
      "Iteration 15, loss = 0.74353883\n",
      "Iteration 16, loss = 0.68592702\n",
      "Iteration 17, loss = 0.62014524\n",
      "Iteration 18, loss = 0.55900670\n",
      "Iteration 19, loss = 0.50809611\n",
      "Iteration 20, loss = 0.46957015\n",
      "Iteration 21, loss = 0.42800797\n",
      "Iteration 22, loss = 0.39524042\n",
      "Iteration 23, loss = 0.36952226\n",
      "Iteration 24, loss = 0.34923137\n",
      "Iteration 25, loss = 0.33342985\n",
      "Iteration 26, loss = 0.32230274\n",
      "Iteration 27, loss = 0.31337895\n",
      "Iteration 28, loss = 0.30889045\n",
      "Iteration 29, loss = 0.30471847\n",
      "Iteration 30, loss = 0.30065784\n",
      "Iteration 31, loss = 0.29839316\n",
      "Iteration 32, loss = 0.29544874\n",
      "Iteration 33, loss = 0.29029793\n",
      "Iteration 34, loss = 0.28806548\n",
      "Iteration 35, loss = 0.28565497\n",
      "Iteration 36, loss = 0.28595345\n",
      "Iteration 37, loss = 0.28424777\n",
      "Iteration 38, loss = 0.28189149\n",
      "Iteration 39, loss = 0.28107616\n",
      "Iteration 40, loss = 0.27930201\n",
      "Iteration 41, loss = 0.27621990\n",
      "Iteration 42, loss = 0.27653241\n",
      "Iteration 43, loss = 0.27461965\n",
      "Iteration 44, loss = 0.27355261\n",
      "Iteration 45, loss = 0.27622640\n",
      "Iteration 46, loss = 0.27621798\n",
      "Iteration 47, loss = 0.26793578\n",
      "Iteration 48, loss = 0.26925136\n",
      "Iteration 49, loss = 0.27338925\n",
      "Iteration 50, loss = 0.27028154\n",
      "Iteration 51, loss = 0.26312146\n",
      "Iteration 52, loss = 0.26243949\n",
      "Iteration 53, loss = 0.25895317\n",
      "Iteration 54, loss = 0.25811776\n",
      "Iteration 55, loss = 0.26255922\n",
      "Iteration 56, loss = 0.25261525\n",
      "Iteration 57, loss = 0.25156577\n",
      "Iteration 58, loss = 0.25356079\n",
      "Iteration 59, loss = 0.24988685\n",
      "Iteration 60, loss = 0.24226292\n",
      "Iteration 61, loss = 0.24237217\n",
      "Iteration 62, loss = 0.24327172\n",
      "Iteration 63, loss = 0.23450708\n",
      "Iteration 64, loss = 0.23138633\n",
      "Iteration 65, loss = 0.22821137\n",
      "Iteration 66, loss = 0.22920002\n",
      "Iteration 67, loss = 0.22942668\n",
      "Iteration 68, loss = 0.22357850\n",
      "Iteration 69, loss = 0.22220435\n",
      "Iteration 70, loss = 0.21810857\n",
      "Iteration 71, loss = 0.21703944\n",
      "Iteration 72, loss = 0.21555242\n",
      "Iteration 73, loss = 0.21345340\n",
      "Iteration 74, loss = 0.21293916\n",
      "Iteration 75, loss = 0.21283354\n",
      "Iteration 76, loss = 0.21113256\n",
      "Iteration 77, loss = 0.21019635\n",
      "Iteration 78, loss = 0.20949227\n",
      "Iteration 79, loss = 0.20804890\n",
      "Iteration 80, loss = 0.20648425\n",
      "Iteration 81, loss = 0.20701408\n",
      "Iteration 82, loss = 0.20495015\n",
      "Iteration 83, loss = 0.20665891\n",
      "Iteration 84, loss = 0.20309110\n",
      "Iteration 85, loss = 0.20451682\n",
      "Iteration 86, loss = 0.20295533\n",
      "Iteration 87, loss = 0.20243364\n",
      "Iteration 88, loss = 0.20278885\n",
      "Iteration 89, loss = 0.20655676\n",
      "Iteration 90, loss = 0.20800017\n",
      "Iteration 91, loss = 0.20673172\n",
      "Iteration 92, loss = 0.20605346\n",
      "Iteration 93, loss = 0.19982320\n",
      "Iteration 94, loss = 0.20350964\n",
      "Iteration 95, loss = 0.19968253\n",
      "Iteration 96, loss = 0.20089288\n",
      "Iteration 97, loss = 0.20088435\n",
      "Iteration 98, loss = 0.20023837\n",
      "Iteration 99, loss = 0.19696032\n",
      "Iteration 100, loss = 0.19914631\n",
      "Iteration 101, loss = 0.19807777\n",
      "Iteration 102, loss = 0.19836872\n",
      "Iteration 103, loss = 0.20171834\n",
      "Iteration 104, loss = 0.19988985\n",
      "Iteration 105, loss = 0.19954903\n",
      "Iteration 106, loss = 0.19753199\n",
      "Iteration 107, loss = 0.19820001\n",
      "Iteration 108, loss = 0.19816622\n",
      "Iteration 109, loss = 0.20156711\n",
      "Iteration 110, loss = 0.19706376\n",
      "Iteration 111, loss = 0.19545915\n",
      "Iteration 112, loss = 0.19699601\n",
      "Iteration 113, loss = 0.19844629\n",
      "Iteration 114, loss = 0.20476454\n",
      "Iteration 115, loss = 0.19648904\n",
      "Iteration 116, loss = 0.19800372\n",
      "Iteration 117, loss = 0.19611206\n",
      "Iteration 118, loss = 0.19624736\n",
      "Iteration 119, loss = 0.19312624\n",
      "Iteration 120, loss = 0.19332734\n",
      "Iteration 121, loss = 0.19394637\n",
      "Iteration 122, loss = 0.19797554\n",
      "Iteration 123, loss = 0.19138086\n",
      "Iteration 124, loss = 0.19195543\n",
      "Iteration 125, loss = 0.19405554\n",
      "Iteration 126, loss = 0.19440820\n",
      "Iteration 127, loss = 0.19038405\n",
      "Iteration 128, loss = 0.19275490\n",
      "Iteration 129, loss = 0.19489999\n",
      "Iteration 130, loss = 0.18983126\n",
      "Iteration 131, loss = 0.18920793\n",
      "Iteration 132, loss = 0.18920727\n",
      "Iteration 133, loss = 0.18934856\n",
      "Iteration 134, loss = 0.18955785\n",
      "Iteration 135, loss = 0.18803995\n",
      "Iteration 136, loss = 0.19002824\n",
      "Iteration 137, loss = 0.18888570\n",
      "Iteration 138, loss = 0.19263774\n",
      "Iteration 139, loss = 0.19549692\n",
      "Iteration 140, loss = 0.19847911\n",
      "Iteration 141, loss = 0.19392424\n",
      "Iteration 142, loss = 0.19293195\n",
      "Iteration 143, loss = 0.18952608\n",
      "Iteration 144, loss = 0.18814298\n",
      "Iteration 145, loss = 0.18647395\n",
      "Iteration 146, loss = 0.18844992\n",
      "Iteration 147, loss = 0.18904555\n",
      "Iteration 148, loss = 0.18869569\n",
      "Iteration 149, loss = 0.19082263\n",
      "Iteration 150, loss = 0.18631036\n",
      "Iteration 151, loss = 0.18895850\n",
      "Iteration 152, loss = 0.18605438\n",
      "Iteration 153, loss = 0.18660568\n",
      "Iteration 154, loss = 0.19081005\n",
      "Iteration 155, loss = 0.18942926\n",
      "Iteration 156, loss = 0.18477712\n",
      "Iteration 157, loss = 0.18645584\n",
      "Iteration 158, loss = 0.19105455\n",
      "Iteration 159, loss = 0.18904701\n",
      "Iteration 160, loss = 0.18431397\n",
      "Iteration 161, loss = 0.18292095\n",
      "Iteration 162, loss = 0.18474099\n",
      "Iteration 163, loss = 0.18612266\n",
      "Iteration 164, loss = 0.18455427\n",
      "Iteration 165, loss = 0.18433985\n",
      "Iteration 166, loss = 0.18183186\n",
      "Iteration 167, loss = 0.18373574\n",
      "Iteration 168, loss = 0.18081738\n",
      "Iteration 169, loss = 0.18301889\n",
      "Iteration 170, loss = 0.18238566\n",
      "Iteration 171, loss = 0.18293223\n",
      "Iteration 172, loss = 0.18077626\n",
      "Iteration 173, loss = 0.18475410\n",
      "Iteration 174, loss = 0.18591433\n",
      "Iteration 175, loss = 0.18096580\n",
      "Iteration 176, loss = 0.17828660\n",
      "Iteration 177, loss = 0.17870845\n",
      "Iteration 178, loss = 0.18198045\n",
      "Iteration 179, loss = 0.17909117\n",
      "Iteration 180, loss = 0.18145848\n",
      "Iteration 181, loss = 0.18217199\n",
      "Iteration 182, loss = 0.18112080\n",
      "Iteration 183, loss = 0.17997116\n",
      "Iteration 184, loss = 0.17748670\n",
      "Iteration 185, loss = 0.17814641\n",
      "Iteration 186, loss = 0.17884094\n",
      "Iteration 187, loss = 0.17890522\n",
      "Iteration 188, loss = 0.18791615\n",
      "Iteration 189, loss = 0.18850542\n",
      "Iteration 190, loss = 0.17477706\n",
      "Iteration 191, loss = 0.18882884\n",
      "Iteration 192, loss = 0.17803412\n",
      "Iteration 193, loss = 0.17951860\n",
      "Iteration 194, loss = 0.17923725\n",
      "Iteration 195, loss = 0.17303583\n",
      "Iteration 196, loss = 0.17571925\n",
      "Iteration 197, loss = 0.18057006\n",
      "Iteration 198, loss = 0.17322975\n",
      "Iteration 199, loss = 0.17803029\n",
      "Iteration 200, loss = 0.17307399\n",
      "Iteration 201, loss = 0.16952288\n",
      "Iteration 202, loss = 0.17410833\n",
      "Iteration 203, loss = 0.17408921\n",
      "Iteration 204, loss = 0.17347291\n",
      "Iteration 205, loss = 0.17252895\n",
      "Iteration 206, loss = 0.16835789\n",
      "Iteration 207, loss = 0.17467807\n",
      "Iteration 208, loss = 0.16728107\n",
      "Iteration 209, loss = 0.16586879\n",
      "Iteration 210, loss = 0.16883038\n",
      "Iteration 211, loss = 0.16588761\n",
      "Iteration 212, loss = 0.16494155\n",
      "Iteration 213, loss = 0.16682109\n",
      "Iteration 214, loss = 0.16731644\n",
      "Iteration 215, loss = 0.16132217\n",
      "Iteration 216, loss = 0.16155178\n",
      "Iteration 217, loss = 0.16101775\n",
      "Iteration 218, loss = 0.16201207\n",
      "Iteration 219, loss = 0.16417900\n",
      "Iteration 220, loss = 0.16010591\n",
      "Iteration 221, loss = 0.15743016\n",
      "Iteration 222, loss = 0.15600396\n",
      "Iteration 223, loss = 0.15878844\n",
      "Iteration 224, loss = 0.15785655\n",
      "Iteration 225, loss = 0.15430465\n",
      "Iteration 226, loss = 0.15470562\n",
      "Iteration 227, loss = 0.15467350\n",
      "Iteration 228, loss = 0.15467677\n",
      "Iteration 229, loss = 0.15659675\n",
      "Iteration 230, loss = 0.15026424\n",
      "Iteration 231, loss = 0.16390040\n",
      "Iteration 232, loss = 0.15097900\n",
      "Iteration 233, loss = 0.14995884\n",
      "Iteration 234, loss = 0.14799158\n",
      "Iteration 235, loss = 0.14555664\n",
      "Iteration 236, loss = 0.14374942\n",
      "Iteration 237, loss = 0.14551842\n",
      "Iteration 238, loss = 0.14919171\n",
      "Iteration 239, loss = 0.14516411\n",
      "Iteration 240, loss = 0.14408910\n",
      "Iteration 241, loss = 0.14072363\n",
      "Iteration 242, loss = 0.14774763\n",
      "Iteration 243, loss = 0.13848682\n",
      "Iteration 244, loss = 0.13687717\n",
      "Iteration 245, loss = 0.14148384\n",
      "Iteration 246, loss = 0.14025781\n",
      "Iteration 247, loss = 0.13641948\n",
      "Iteration 248, loss = 0.13344672\n",
      "Iteration 249, loss = 0.13079240\n",
      "Iteration 250, loss = 0.13000636\n",
      "Iteration 251, loss = 0.12930265\n",
      "Iteration 252, loss = 0.12939296\n",
      "Iteration 253, loss = 0.13304496\n",
      "Iteration 254, loss = 0.12974869\n",
      "Iteration 255, loss = 0.12780408\n",
      "Iteration 256, loss = 0.12648048\n",
      "Iteration 257, loss = 0.12947836\n",
      "Iteration 258, loss = 0.12176704\n",
      "Iteration 259, loss = 0.12451615\n",
      "Iteration 260, loss = 0.11913994\n",
      "Iteration 261, loss = 0.12124386\n",
      "Iteration 262, loss = 0.11785201\n",
      "Iteration 263, loss = 0.11603960\n",
      "Iteration 264, loss = 0.11520762\n",
      "Iteration 265, loss = 0.11595490\n",
      "Iteration 266, loss = 0.11782276\n",
      "Iteration 267, loss = 0.11851779\n",
      "Iteration 268, loss = 0.11169829\n",
      "Iteration 269, loss = 0.11741921\n",
      "Iteration 270, loss = 0.11630007\n",
      "Iteration 271, loss = 0.11147023\n",
      "Iteration 272, loss = 0.10719929\n",
      "Iteration 273, loss = 0.10595442\n",
      "Iteration 274, loss = 0.10604922\n",
      "Iteration 275, loss = 0.10562704\n",
      "Iteration 276, loss = 0.10687135\n",
      "Iteration 277, loss = 0.11255884\n",
      "Iteration 278, loss = 0.10076054\n",
      "Iteration 279, loss = 0.10264094\n",
      "Iteration 280, loss = 0.09999818\n",
      "Iteration 281, loss = 0.09918035\n",
      "Iteration 282, loss = 0.10574707\n",
      "Iteration 283, loss = 0.11266150\n",
      "Iteration 284, loss = 0.09719706\n",
      "Iteration 285, loss = 0.09529996\n",
      "Iteration 286, loss = 0.09650834\n",
      "Iteration 287, loss = 0.09611578\n",
      "Iteration 288, loss = 0.09289781\n",
      "Iteration 289, loss = 0.09395568\n",
      "Iteration 290, loss = 0.09095123\n",
      "Iteration 291, loss = 0.09212237\n",
      "Iteration 292, loss = 0.09314618\n",
      "Iteration 293, loss = 0.09757702\n",
      "Iteration 294, loss = 0.09385352\n",
      "Iteration 295, loss = 0.08838757\n",
      "Iteration 296, loss = 0.08802225\n",
      "Iteration 297, loss = 0.08693189\n",
      "Iteration 298, loss = 0.08488344\n",
      "Iteration 299, loss = 0.08442849\n",
      "Iteration 300, loss = 0.08475559\n",
      "Iteration 301, loss = 0.08630986\n",
      "Iteration 302, loss = 0.08394411\n",
      "Iteration 303, loss = 0.08459177\n",
      "Iteration 304, loss = 0.08434847\n",
      "Iteration 305, loss = 0.07894971\n",
      "Iteration 306, loss = 0.08090950\n",
      "Iteration 307, loss = 0.07747510\n",
      "Iteration 308, loss = 0.07802746\n",
      "Iteration 309, loss = 0.07741889\n",
      "Iteration 310, loss = 0.07645685\n",
      "Iteration 311, loss = 0.07463259\n",
      "Iteration 312, loss = 0.07898218\n",
      "Iteration 313, loss = 0.07638698\n",
      "Iteration 314, loss = 0.08019293\n",
      "Iteration 315, loss = 0.07573035\n",
      "Iteration 316, loss = 0.07130963\n",
      "Iteration 317, loss = 0.07008098\n",
      "Iteration 318, loss = 0.06821760\n",
      "Iteration 319, loss = 0.06812704\n",
      "Iteration 320, loss = 0.07103166\n",
      "Iteration 321, loss = 0.06870105\n",
      "Iteration 322, loss = 0.07377095\n",
      "Iteration 323, loss = 0.06710504\n",
      "Iteration 324, loss = 0.06874364\n",
      "Iteration 325, loss = 0.06627950\n",
      "Iteration 326, loss = 0.06818062\n",
      "Iteration 327, loss = 0.06741258\n",
      "Iteration 328, loss = 0.06706414\n",
      "Iteration 329, loss = 0.06288040\n",
      "Iteration 330, loss = 0.06656125\n",
      "Iteration 331, loss = 0.06060134\n",
      "Iteration 332, loss = 0.06277309\n",
      "Iteration 333, loss = 0.06829398\n",
      "Iteration 334, loss = 0.06551370\n",
      "Iteration 335, loss = 0.05731064\n",
      "Iteration 336, loss = 0.05686992\n",
      "Iteration 337, loss = 0.05758221\n",
      "Iteration 338, loss = 0.05558678\n",
      "Iteration 339, loss = 0.05522377\n",
      "Iteration 340, loss = 0.05482325\n",
      "Iteration 341, loss = 0.05469343\n",
      "Iteration 342, loss = 0.06143696\n",
      "Iteration 343, loss = 0.06337765\n",
      "Iteration 344, loss = 0.05280410\n",
      "Iteration 345, loss = 0.05314501\n",
      "Iteration 346, loss = 0.05106108\n",
      "Iteration 347, loss = 0.06010229\n",
      "Iteration 348, loss = 0.05824610\n",
      "Iteration 349, loss = 0.04707871\n",
      "Iteration 350, loss = 0.05351099\n",
      "Iteration 351, loss = 0.04640024\n",
      "Iteration 352, loss = 0.05051137\n",
      "Iteration 353, loss = 0.04830614\n",
      "Iteration 354, loss = 0.04555772\n",
      "Iteration 355, loss = 0.04432650\n",
      "Iteration 356, loss = 0.04323443\n",
      "Iteration 357, loss = 0.04361333\n",
      "Iteration 358, loss = 0.04281684\n",
      "Iteration 359, loss = 0.04224206\n",
      "Iteration 360, loss = 0.04119755\n",
      "Iteration 361, loss = 0.04164682\n",
      "Iteration 362, loss = 0.03968507\n",
      "Iteration 363, loss = 0.03957141\n",
      "Iteration 364, loss = 0.04042373\n",
      "Iteration 365, loss = 0.04024150\n",
      "Iteration 366, loss = 0.03923513\n",
      "Iteration 367, loss = 0.03794064\n",
      "Iteration 368, loss = 0.03770194\n",
      "Iteration 369, loss = 0.03750985\n",
      "Iteration 370, loss = 0.03601902\n",
      "Iteration 371, loss = 0.03875688\n",
      "Iteration 372, loss = 0.04393754\n",
      "Iteration 373, loss = 0.03464885\n",
      "Iteration 374, loss = 0.03540114\n",
      "Iteration 375, loss = 0.03530033\n",
      "Iteration 376, loss = 0.03560288\n",
      "Iteration 377, loss = 0.03544186\n",
      "Iteration 378, loss = 0.03176705\n",
      "Iteration 379, loss = 0.03153448\n",
      "Iteration 380, loss = 0.03133197\n",
      "Iteration 381, loss = 0.03241709\n",
      "Iteration 382, loss = 0.03416316\n",
      "Iteration 383, loss = 0.03741763\n",
      "Iteration 384, loss = 0.03516779\n",
      "Iteration 385, loss = 0.03840631\n",
      "Iteration 386, loss = 0.03122031\n",
      "Iteration 387, loss = 0.02907084\n",
      "Iteration 388, loss = 0.02859005\n",
      "Iteration 389, loss = 0.02878109\n",
      "Iteration 390, loss = 0.02871276\n",
      "Iteration 391, loss = 0.02722416\n",
      "Iteration 392, loss = 0.02668673\n",
      "Iteration 393, loss = 0.02656777\n",
      "Iteration 394, loss = 0.02623634\n",
      "Iteration 395, loss = 0.02907878\n",
      "Iteration 396, loss = 0.02804999\n",
      "Iteration 397, loss = 0.02548857\n",
      "Iteration 398, loss = 0.02664994\n",
      "Iteration 399, loss = 0.02849611\n",
      "Iteration 400, loss = 0.02588319\n",
      "Iteration 401, loss = 0.02761578\n",
      "Iteration 402, loss = 0.03757195\n",
      "Iteration 403, loss = 0.02401778\n",
      "Iteration 404, loss = 0.02305208\n",
      "Iteration 405, loss = 0.02246086\n",
      "Iteration 406, loss = 0.02263447\n",
      "Iteration 407, loss = 0.02381153\n",
      "Iteration 408, loss = 0.02777083\n",
      "Iteration 409, loss = 0.02246360\n",
      "Iteration 410, loss = 0.02208945\n",
      "Iteration 411, loss = 0.02251456\n",
      "Iteration 412, loss = 0.02280549\n",
      "Iteration 413, loss = 0.02404591\n",
      "Iteration 414, loss = 0.02199180\n",
      "Iteration 415, loss = 0.02587334\n",
      "Iteration 416, loss = 0.02202452\n",
      "Iteration 417, loss = 0.02227440\n",
      "Iteration 418, loss = 0.02019940\n",
      "Iteration 419, loss = 0.01920888\n",
      "Iteration 420, loss = 0.02019603\n",
      "Iteration 421, loss = 0.02321861\n",
      "Iteration 422, loss = 0.02104547\n",
      "Iteration 423, loss = 0.01927747\n",
      "Iteration 424, loss = 0.01844438\n",
      "Iteration 425, loss = 0.01805034\n",
      "Iteration 426, loss = 0.01862956\n",
      "Iteration 427, loss = 0.01864859\n",
      "Iteration 428, loss = 0.01799575\n",
      "Iteration 429, loss = 0.01883175\n",
      "Iteration 430, loss = 0.01969214\n",
      "Iteration 431, loss = 0.01779149\n",
      "Iteration 432, loss = 0.01716721\n",
      "Iteration 433, loss = 0.01791738\n",
      "Iteration 434, loss = 0.01785585\n",
      "Iteration 435, loss = 0.01747118\n",
      "Iteration 436, loss = 0.01727960\n",
      "Iteration 437, loss = 0.01651089\n",
      "Iteration 438, loss = 0.01602956\n",
      "Iteration 439, loss = 0.01747868\n",
      "Iteration 440, loss = 0.01982010\n",
      "Iteration 441, loss = 0.01597562\n",
      "Iteration 442, loss = 0.01541351\n",
      "Iteration 443, loss = 0.01550946\n",
      "Iteration 444, loss = 0.01572054\n",
      "Iteration 445, loss = 0.01637399\n",
      "Iteration 446, loss = 0.01634123\n",
      "Iteration 447, loss = 0.01498182\n",
      "Iteration 448, loss = 0.01553151\n",
      "Iteration 449, loss = 0.01578801\n",
      "Iteration 450, loss = 0.01598707\n",
      "Iteration 451, loss = 0.01570547\n",
      "Iteration 452, loss = 0.01410611\n",
      "Iteration 453, loss = 0.01456018\n",
      "Iteration 454, loss = 0.01412409\n",
      "Iteration 455, loss = 0.01373519\n",
      "Iteration 456, loss = 0.01374595\n",
      "Iteration 457, loss = 0.01414438\n",
      "Iteration 458, loss = 0.01407813\n",
      "Iteration 459, loss = 0.01425689\n",
      "Iteration 460, loss = 0.01393927\n",
      "Iteration 461, loss = 0.01420607\n",
      "Iteration 462, loss = 0.01418019\n",
      "Iteration 463, loss = 0.01329770\n",
      "Iteration 464, loss = 0.01427651\n",
      "Iteration 465, loss = 0.01470014\n",
      "Iteration 466, loss = 0.01414623\n",
      "Iteration 467, loss = 0.01433253\n",
      "Iteration 468, loss = 0.01259694\n",
      "Iteration 469, loss = 0.01263440\n",
      "Iteration 470, loss = 0.01233715\n",
      "Iteration 471, loss = 0.01267601\n",
      "Iteration 472, loss = 0.01365745\n",
      "Iteration 473, loss = 0.01283097\n",
      "Iteration 474, loss = 0.01223450\n",
      "Iteration 475, loss = 0.01309563\n",
      "Iteration 476, loss = 0.01173307\n",
      "Iteration 477, loss = 0.01230581\n",
      "Iteration 478, loss = 0.01157252\n",
      "Iteration 479, loss = 0.01158415\n",
      "Iteration 480, loss = 0.01153773\n",
      "Iteration 481, loss = 0.01135302\n",
      "Iteration 482, loss = 0.01149187\n",
      "Iteration 483, loss = 0.01118639\n",
      "Iteration 484, loss = 0.01120513\n",
      "Iteration 485, loss = 0.01114570\n",
      "Iteration 486, loss = 0.01094991\n",
      "Iteration 487, loss = 0.01089553\n",
      "Iteration 488, loss = 0.01099432\n",
      "Iteration 489, loss = 0.01126479\n",
      "Iteration 490, loss = 0.01096729\n",
      "Iteration 491, loss = 0.01057310\n",
      "Iteration 492, loss = 0.01092973\n",
      "Iteration 493, loss = 0.01047930\n",
      "Iteration 494, loss = 0.01087103\n",
      "Iteration 495, loss = 0.01076794\n",
      "Iteration 496, loss = 0.01037469\n",
      "Iteration 497, loss = 0.01021736\n",
      "Iteration 498, loss = 0.01058179\n",
      "Iteration 499, loss = 0.01119100\n",
      "Iteration 500, loss = 0.01013059\n",
      "Iteration 501, loss = 0.01048300\n",
      "Iteration 502, loss = 0.01107173\n",
      "Iteration 503, loss = 0.01090597\n",
      "Iteration 504, loss = 0.01121459\n",
      "Iteration 505, loss = 0.01034208\n",
      "Iteration 506, loss = 0.01005380\n",
      "Iteration 507, loss = 0.00992384\n",
      "Iteration 508, loss = 0.00970963\n",
      "Iteration 509, loss = 0.01044319\n",
      "Iteration 510, loss = 0.01080559\n",
      "Iteration 511, loss = 0.00989245\n",
      "Iteration 512, loss = 0.00997186\n",
      "Iteration 513, loss = 0.00994019\n",
      "Iteration 514, loss = 0.00932969\n",
      "Iteration 515, loss = 0.00947830\n",
      "Iteration 516, loss = 0.00984756\n",
      "Iteration 517, loss = 0.00928627\n",
      "Iteration 518, loss = 0.00915728\n",
      "Iteration 519, loss = 0.00937303\n",
      "Iteration 520, loss = 0.00981812\n",
      "Iteration 521, loss = 0.00922912\n",
      "Iteration 522, loss = 0.00905811\n",
      "Iteration 523, loss = 0.00910137\n",
      "Iteration 524, loss = 0.00918130\n",
      "Iteration 525, loss = 0.00893614\n",
      "Iteration 526, loss = 0.00913486\n",
      "Iteration 527, loss = 0.00877359\n",
      "Iteration 528, loss = 0.00891696\n",
      "Iteration 529, loss = 0.00884976\n",
      "Iteration 530, loss = 0.00921271\n",
      "Iteration 531, loss = 0.00869967\n",
      "Iteration 532, loss = 0.00866594\n",
      "Iteration 533, loss = 0.00890101\n",
      "Iteration 534, loss = 0.00859223\n",
      "Iteration 535, loss = 0.00857155\n",
      "Iteration 536, loss = 0.00845641\n",
      "Iteration 537, loss = 0.00846957\n",
      "Iteration 538, loss = 0.00861317\n",
      "Iteration 539, loss = 0.00860236\n",
      "Iteration 540, loss = 0.00869222\n",
      "Iteration 541, loss = 0.00838035\n",
      "Iteration 542, loss = 0.00843768\n",
      "Iteration 543, loss = 0.00835267\n",
      "Iteration 544, loss = 0.00843862\n",
      "Iteration 545, loss = 0.00841920\n",
      "Iteration 546, loss = 0.00831029\n",
      "Iteration 547, loss = 0.00882285\n",
      "Iteration 548, loss = 0.00883846\n",
      "Iteration 549, loss = 0.00858955\n",
      "Iteration 550, loss = 0.00835796\n",
      "Iteration 551, loss = 0.00811910\n",
      "Iteration 552, loss = 0.00815869\n",
      "Iteration 553, loss = 0.00803384\n",
      "Iteration 554, loss = 0.00821293\n",
      "Iteration 555, loss = 0.00824914\n",
      "Iteration 556, loss = 0.00836905\n",
      "Iteration 557, loss = 0.00804528\n",
      "Iteration 558, loss = 0.00798827\n",
      "Iteration 559, loss = 0.00787469\n",
      "Iteration 560, loss = 0.00798755\n",
      "Iteration 561, loss = 0.00805415\n",
      "Iteration 562, loss = 0.00817895\n",
      "Iteration 563, loss = 0.00808760\n",
      "Iteration 564, loss = 0.00780031\n",
      "Iteration 565, loss = 0.00771165\n",
      "Iteration 566, loss = 0.00775461\n",
      "Iteration 567, loss = 0.00769060\n",
      "Iteration 568, loss = 0.00765121\n",
      "Iteration 569, loss = 0.00772011\n",
      "Iteration 570, loss = 0.00771366\n",
      "Iteration 571, loss = 0.00759615\n",
      "Iteration 572, loss = 0.00778592\n",
      "Iteration 573, loss = 0.00777145\n",
      "Iteration 574, loss = 0.00782426\n",
      "Iteration 575, loss = 0.00748444\n",
      "Iteration 576, loss = 0.00757244\n",
      "Iteration 577, loss = 0.00752974\n",
      "Iteration 578, loss = 0.00746193\n",
      "Iteration 579, loss = 0.00748956\n",
      "Iteration 580, loss = 0.00757315\n",
      "Iteration 581, loss = 0.00751317\n",
      "Iteration 582, loss = 0.00756300\n",
      "Iteration 583, loss = 0.00759930\n",
      "Iteration 584, loss = 0.00767194\n",
      "Iteration 585, loss = 0.00739301\n",
      "Iteration 586, loss = 0.00728801\n",
      "Iteration 587, loss = 0.00732272\n",
      "Iteration 588, loss = 0.00732470\n",
      "Iteration 589, loss = 0.00733864\n",
      "Iteration 590, loss = 0.00720161\n",
      "Iteration 591, loss = 0.00728891\n",
      "Iteration 592, loss = 0.00733782\n",
      "Iteration 593, loss = 0.00731838\n",
      "Iteration 594, loss = 0.00721313\n",
      "Iteration 595, loss = 0.00713463\n",
      "Iteration 596, loss = 0.00725521\n",
      "Iteration 597, loss = 0.00724250\n",
      "Iteration 598, loss = 0.00713715\n",
      "Iteration 599, loss = 0.00710277\n",
      "Iteration 600, loss = 0.00711309\n",
      "Iteration 601, loss = 0.00717579\n",
      "Iteration 602, loss = 0.00705747\n",
      "Iteration 603, loss = 0.00717911\n",
      "Iteration 604, loss = 0.00717405\n",
      "Iteration 605, loss = 0.00700216\n",
      "Iteration 606, loss = 0.00732209\n",
      "Iteration 607, loss = 0.00738884\n",
      "Iteration 608, loss = 0.00691460\n",
      "Iteration 609, loss = 0.00745238\n",
      "Iteration 610, loss = 0.00690421\n",
      "Iteration 611, loss = 0.00721493\n",
      "Iteration 612, loss = 0.00692275\n",
      "Iteration 613, loss = 0.00690825\n",
      "Iteration 614, loss = 0.00689961\n",
      "Iteration 615, loss = 0.00699659\n",
      "Iteration 616, loss = 0.00698405\n",
      "Iteration 617, loss = 0.00700628\n",
      "Iteration 618, loss = 0.00683409\n",
      "Iteration 619, loss = 0.00682613\n",
      "Iteration 620, loss = 0.00702734\n",
      "Iteration 621, loss = 0.00682082\n",
      "Iteration 622, loss = 0.00721309\n",
      "Iteration 623, loss = 0.00689493\n",
      "Iteration 624, loss = 0.00674409\n",
      "Iteration 625, loss = 0.00680574\n",
      "Iteration 626, loss = 0.00678424\n",
      "Iteration 627, loss = 0.00675680\n",
      "Iteration 628, loss = 0.00672601\n",
      "Iteration 629, loss = 0.00674004\n",
      "Iteration 630, loss = 0.00670534\n",
      "Iteration 631, loss = 0.00671378\n",
      "Iteration 632, loss = 0.00671388\n",
      "Iteration 633, loss = 0.00664991\n",
      "Iteration 634, loss = 0.00665880\n",
      "Iteration 635, loss = 0.00668396\n",
      "Iteration 636, loss = 0.00660397\n",
      "Iteration 637, loss = 0.00680103\n",
      "Iteration 638, loss = 0.00675590\n",
      "Iteration 639, loss = 0.00657702\n",
      "Iteration 640, loss = 0.00657316\n",
      "Iteration 641, loss = 0.00659963\n",
      "Iteration 642, loss = 0.00654994\n",
      "Iteration 643, loss = 0.00653373\n",
      "Iteration 644, loss = 0.00655002\n",
      "Iteration 645, loss = 0.00652141\n",
      "Iteration 646, loss = 0.00653123\n",
      "Iteration 647, loss = 0.00660925\n",
      "Iteration 648, loss = 0.00648991\n",
      "Iteration 649, loss = 0.00660035\n",
      "Iteration 650, loss = 0.00669619\n",
      "Iteration 651, loss = 0.00651402\n",
      "Iteration 652, loss = 0.00687105\n",
      "Iteration 653, loss = 0.00653666\n",
      "Iteration 654, loss = 0.00656560\n",
      "Iteration 655, loss = 0.00641301\n",
      "Iteration 656, loss = 0.00647899\n",
      "Iteration 657, loss = 0.00650346\n",
      "Iteration 658, loss = 0.00645689\n",
      "Iteration 659, loss = 0.00638502\n",
      "Iteration 660, loss = 0.00637556\n",
      "Iteration 661, loss = 0.00639341\n",
      "Iteration 662, loss = 0.00636813\n",
      "Iteration 663, loss = 0.00634819\n",
      "Iteration 664, loss = 0.00636724\n",
      "Iteration 665, loss = 0.00644411\n",
      "Iteration 666, loss = 0.00634888\n",
      "Iteration 667, loss = 0.00633266\n",
      "Iteration 668, loss = 0.00630485\n",
      "Iteration 669, loss = 0.00629378\n",
      "Iteration 670, loss = 0.00630258\n",
      "Iteration 671, loss = 0.00632033\n",
      "Iteration 672, loss = 0.00633291\n",
      "Iteration 673, loss = 0.00633650\n",
      "Iteration 674, loss = 0.00626864\n",
      "Iteration 675, loss = 0.00631662\n",
      "Iteration 676, loss = 0.00630325\n",
      "Iteration 677, loss = 0.00626698\n",
      "Iteration 678, loss = 0.00623661\n",
      "Iteration 679, loss = 0.00629030\n",
      "Iteration 680, loss = 0.00625985\n",
      "Iteration 681, loss = 0.00632907\n",
      "Iteration 682, loss = 0.00627027\n",
      "Iteration 683, loss = 0.00632177\n",
      "Iteration 684, loss = 0.00628906\n",
      "Iteration 685, loss = 0.00619271\n",
      "Iteration 686, loss = 0.00618789\n",
      "Iteration 687, loss = 0.00618914\n",
      "Training loss did not improve more than tol=0.000100 for 100 consecutive epochs. Stopping.\n",
      "-> Validacion SKLearn MLP: 86.52%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- CONFIGURACIÓN (TOCA AQUÍ SI NO LLEGAS A LA NOTA) ---\n",
    "# Si Accuracy bajo -> Sube capas (ej: (100, 50)) o sube ITERATIONS\n",
    "# Si tarda mucho -> Sube LR_INIT (ej: 0.01)\n",
    "LAYERS = (64, 32)\n",
    "LR_INIT = 0.01   # Con SGD a veces hay que subirlo un poco\n",
    "ITERATIONS = 5000\n",
    "\n",
    "print(\"--- Entrenando MLP Sklearn (SGD)... ---\")\n",
    "mlp_sklearn = MLPClassifier(\n",
    "    hidden_layer_sizes=LAYERS,\n",
    "    activation='logistic',  # Sigmoide\n",
    "    solver='sgd',           # Descenso de gradiente clásico\n",
    "    max_iter=ITERATIONS,\n",
    "    learning_rate_init=LR_INIT,\n",
    "    random_state=42,\n",
    "    verbose=False           # Pon True si quieres ver si avanza\n",
    ")\n",
    "\n",
    "mlp_sklearn.fit(X_train, y_train)\n",
    "y_pred = mlp_sklearn.predict(X_test)\n",
    "\n",
    "# --- RESULTADOS ---\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"✅ Accuracy MLP (SGD): {acc*100:.2f}%\")\n",
    "\n",
    "# Matriz de Confusión\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Matriz de Confusión - MLP SGD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5469eb",
   "metadata": {},
   "source": [
    "### MLP Sklearn \"OPTIMIZADO\" (Adam + ReLU)\n",
    "\n",
    "si piden \"Máximo rendimiento\" o un accuracy muy alto (>85%). Es la más potente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e7de1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- CONFIGURACIÓN PRO ---\n",
    "# ADAM + RELU convergen muchísimo más rápido y mejor.\n",
    "LAYERS = (100, 50) \n",
    "\n",
    "print(\"--- Entrenando MLP Optimizado (Adam + ReLU)... ---\")\n",
    "mlp_opt = MLPClassifier(\n",
    "    hidden_layer_sizes=LAYERS,\n",
    "    activation='relu',      # ¡CLAVE! ReLU evita desvanecimiento de gradiente\n",
    "    solver='adam',          # ¡CLAVE! Adam optimiza el learning rate solo\n",
    "    max_iter=5000,\n",
    "    alpha=0.0001,           # Regularización L2 (sube si hay overfitting)\n",
    "    random_state=42,\n",
    "    early_stopping=True     # Para si deja de mejorar (ahorra tiempo)\n",
    ")\n",
    "\n",
    "mlp_opt.fit(X_train, y_train)\n",
    "y_pred = mlp_opt.predict(X_test)\n",
    "\n",
    "# --- RESULTADOS ---\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"🚀 Accuracy MLP Optimizado: {acc*100:.2f}%\")\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Greens')\n",
    "plt.title(\"Matriz de Confusión - MLP Adam\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11841086",
   "metadata": {},
   "source": [
    "### KNN\n",
    "\n",
    "   | pros | cons | \n",
    "   |---|---|\n",
    "   | caja blanca | sensible a irrelevancias y ruido |\n",
    "   | entrenamiento rapido | ejecucion lenta |\n",
    "   | tolerancia a la forma de los datos | costoso en memoria |\n",
    "\n",
    "Si tienes muchos datos es lento. Si tienes muchas dimensiones funciona regular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6920e659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Validacion KNN: 82.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ines\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_classification.py:239: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- CONFIGURACIÓN ---\n",
    "# n_neighbors: Típico 3, 5, 7. \n",
    "# Si el número es muy bajo (1) -> Overfitting (ruido).\n",
    "# Si es muy alto (20) -> Underfitting (demasiado suave).\n",
    "K = 5\n",
    "\n",
    "print(f\"--- Entrenando KNN (k={K})... ---\")\n",
    "knn = KNeighborsClassifier(n_neighbors=K)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# --- RESULTADOS ---\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"📍 Accuracy KNN: {acc*100:.2f}%\")\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Reds')\n",
    "plt.title(f\"Matriz de Confusión - KNN ({K})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed39e3fc",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "\n",
    "   | pros | cons | \n",
    "   |---|---|\n",
    "   | caja blanca | sensible al ruido |\n",
    "   | entrenamiento rapido | simple |\n",
    "   | agradecido con los datos | tiende al overfitting si no hay limite de profunidad |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594211dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Validacion DT: 80.90%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- CONFIGURACIÓN (TOCA AQUÍ) ---\n",
    "# Si Accuracy bajo (Underfitting) -> SUBE max_depth (ej: 15, 20 o None)\n",
    "# Si Overfitting (Train 100% pero Test bajo) -> BAJA max_depth (ej: 5, 8)\n",
    "DEPTH = 10 \n",
    "\n",
    "print(f\"--- Entrenando Árbol de Decisión (Depth={DEPTH})... ---\")\n",
    "dt = DecisionTreeClassifier(max_depth=DEPTH, random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# --- RESULTADOS ---\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"🌳 Accuracy Decision Tree: {acc*100:.2f}%\")\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Oranges')\n",
    "plt.title(\"Matriz de Confusión - Decision Tree\")\n",
    "plt.show()\n",
    "\n",
    "# Opcional: Ver importancia de variables\n",
    "# print(\"Importancia:\", dict(zip(final_data.columns[:-1], dt.feature_importances_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25777d0d",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "   | pros | cons | \n",
    "   |---|---|\n",
    "   | facil de calcular | caja gris (blanca por el dt, negra por la votacion) |\n",
    "   | valora la relevancia de los datos | sensible al ruido, tiende al overfitting |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2d1fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Validacion RF: 89.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ines\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1365: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- CONFIGURACIÓN ---\n",
    "# n_estimators: Número de árboles (100 suele bastar, sube a 200 si necesitas más estabilidad)\n",
    "# max_depth: Igual que en el árbol, controla la complejidad.\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=15, random_state=42)\n",
    "\n",
    "print(\"--- Entrenando Random Forest... ---\")\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# --- RESULTADOS ---\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"🌲🌲 Accuracy Random Forest: {acc*100:.2f}%\")\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Purples')\n",
    "plt.title(\"Matriz de Confusión - Random Forest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2740c69",
   "metadata": {},
   "source": [
    "## Ejercicio 5: Metricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac4716",
   "metadata": {},
   "source": [
    "Accuracy o Precisión (tasa de exactitud): \n",
    "$\\Large\\frac{TP+TN}{TP+N+FP+FN} = \\frac{V}{F}$\n",
    "\n",
    "Recall (ratio de positivos reales): $\\Large\\frac{TP}{TP+FN}$\n",
    "\n",
    "Precision (ratio de clasificaciones correctas) : $\\Large\\frac{TP}{TP+FP}$\n",
    "\n",
    "\n",
    "   | ↓ Datos predichos | Positive Observed | Negative Observed | ← Datos reales |\n",
    "   |---|---|---|---|\n",
    "   | Positive Predicted | TP | FP | Precision |\n",
    "   | Negative Predicted | FP | TN |   |\n",
    "   | ↑ Datos predichos | Recall |  | |\n",
    "\n",
    "F1Score: $\\Large\\frac{Precision*Recall}{Precision+Recall}$\n",
    "\n",
    "Mide el equilibrio entre el Recall y el Precision entre 0 y 1, 1 seria un clasificador perfecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d575bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Accuracy/Precisión Custom MLP: 85.39%\n",
      "-> Recall Custom MLP: 85.39%\n",
      "-> Precision Custom MLP: 73.02%\n",
      "-> F1 Score Custom MLP: 71.88%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Matriz de confusion (con una)\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_sklearn, cmap=\"Reds\", colorbar=False)\n",
    "\n",
    "# ACCURACY\n",
    "acc_custom = accuracy_score(y_test, y_pred)\n",
    "print(f\"-> Accuracy/Precisión Custom MLP: {acc_custom * 100:.2f}%\")\n",
    "\n",
    "# RECALL\n",
    "recall_custom = recall_score(y_test, y_pred, average='micro')\n",
    "print(f\"-> Recall Custom MLP: {recall_custom * 100:.2f}%\")\n",
    "\n",
    "# PRECISION\n",
    "prec_custom = precision_score(y_test, y_pred, average='macro')\n",
    "print(f\"-> Precision Custom MLP: {prec_custom * 100:.2f}%\")\n",
    "\n",
    "# F1 Score\n",
    "f1_custom = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"-> F1 Score Custom MLP: {f1_custom * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676c1a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'SKLearn\\nAcc: 80.90%')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Matriz de confusion (con varias)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_sklearn, ax=axes[0], cmap=\"Reds\", colorbar=False)\n",
    "axes[0].set_title(f\"MLP\\nAcc: {acc_custom * 100:.2f}%\")\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_dt, ax=axes[1], cmap=\"Reds\", colorbar=False)\n",
    "axes[1].set_title(f\"SKLearn\\nAcc: {acc_dt * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66de02b1",
   "metadata": {},
   "source": [
    "### DIAGNOSTICO\n",
    "\n",
    "   | (coste) | validacion baja | validacion alta | \n",
    "   |---|---|---|\n",
    "   | entrenamiento bajo | precision alta (lo que queremos) | overfitting (overfitting, tmb se puede ir subiendo lambda con cuidado) |\n",
    "   | entrenamiento alto | datos de validacion sesgados (modelo mal) | modelo mal |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
